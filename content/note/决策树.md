---
title: 决策树
author: Xin Lu
date: '2023-07-04'
slug: []
categories: []
tags: []
---

实例

hunt 算法是许多决策树算法的基础，包括 ID3、C4.5、CART等。Hunt 算法通过递归方式建立决策树



决策树算法有多个变体和扩展，以下是其中一些常见的决策树算法：

1. ID3（Iterative Dichotomiser 3）：ID3是最早的决策树算法之一，它使用信息增益（Information Gain）作为特征选择准则。但ID3存在一个问题，就是对具有较多取值的特征有偏好。

   ````
   信息增益准则对可取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5算法采用信息增益率来选择最优划分属性
   
   选取信息增益大的属性作为分裂节点
   ````

   

2. C4.5：C4.5是ID3的改进版本，它使用信息增益比（Gain Ratio）作为特征选择准则，解决了ID3对具有较多取值的特征的偏好问题。此外，C4.5还能处理缺失值，并支持数值型特征的离散化。

```
C4.5算法选择具有最大信息增益比的特征作为当前节点的分裂准则
```



1. CART（Classification and Regression Trees）：CART是一种用于分类和回归的决策树算法。对于分类问题，CART使用基尼指数（Gini Index）作为特征选择准则；对于回归问题，CART使用平方误差最小化作为特征选择准则。

```
基尼指数衡量了数据集的不纯度或混乱程度。基尼指数越小，表示数据集的纯度越高，样本的类别分布越集中,
在CART算法中，我们希望选择基尼指数较小的特征作为分裂准则
```



1. RandomForest（随机森林）：随机森林是一种集成学习方法，它通过构建多个决策树来进行分类或回归。每个决策树由随机抽样得到的训练集构建，最后的预测结果由多个决策树的预测结果进行集成。
2. GBDT（Gradient Boosting Decision Tree）：GBDT是一种迭代的决策树算法，它通过逐步提升每棵树的拟合能力来提高整体模型的性能。在GBDT中，每棵树都试图纠正上一棵树的残差，从而逐步减小预测误差。