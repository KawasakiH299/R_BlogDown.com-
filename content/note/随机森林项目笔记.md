---
title: 随机森林项目笔记
author: Xin Lu
date: '2023-05-12'
slug: []
categories: []
tags: []
---

```
#将columns的索引转化为小写
data_train.columns.str.lower()
```

---

[pandas](https://so.csdn.net/so/search?q=pandas&spm=1001.2101.3001.7020)中fillna()方法，能够使用指定的方法填充NA/NaN值。

---

```
#忽略警告信息
import warnings
warnings.filterwarnings("ignore")
```

---

pandas中apply函数

```
import numpy as np
import pandas as pd
 
 
f = lambda x: x.max()-x.min()
 
df = pd.DataFrame(np.random.randn(4,3),columns=list('bde'),index=['utah', 'ohio', 'texas', 'oregon'])
print(df)
 
t1 = df.apply(f)
print(t1)
 
t2 = df.apply(f, axis=1)
print(t2)
```

输出结果：

```
               b         d         e
utah    1.106486  0.101113 -0.494279
ohio    0.955676 -1.889499  0.522151
texas   1.891144 -0.670588  0.106530
oregon -0.062372  0.991231  0.294464
 
b    1.953516
d    2.880730
e    1.016430
dtype: float64
 
utah      1.600766
ohio      2.845175
texas     2.561732
oregon    1.053603
dtype: float64
```

----

##### [Python之lambda匿名函数使用if条件语句](https://www.cnblogs.com/jums/p/10672114.html)

```
C0LOR1 = "PapayaWhip"
COLOR2 = "Tan"
 
selectColor = lambda num: C0LOR1 if num % 2 == 0 else COLOR2
# 使用
COLOR = selectColor(1) # "Tan"
```

解析：当num的值是偶数时显示COLOR1，否则显示COLOR2

---

https://blog.csdn.net/weixin_45875105/article/details/107818766  python sklearn 编码(one-hot,标签编码)

https://www.cnblogs.com/sench/p/10134094.html  sklearn.preprocessing.LabelEncoder的使用

https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html  官方解释

 

1. 准备好数据，使用LabelEncoder对数据集进行编码

  代码实例：

  ```
  from sklearn import preprocessing
   
  data=['小猫','小猫','小狗','小狗','兔子','兔子']  #准备好数据
   
  #方法1：
  enc=preprocessing.LabelEncoder()   #获取一个LabelEncoder
  enc=enc.fit(['小猫','小狗','兔子'])  #训练LabelEncoder
  data=enc.transform(data)       #使用训练好的LabelEncoder对原数据进行编码
   
  #方法2：
  #enc=preprocessing.LabelEncoder()   #获取一个LabelEncoder
  #data=enc.fit_transform(data) 
   
   
  print(data)    #输出编码后的数据
   
  输出：[2 2 1 1 0 0]
  ```

  ```
  >>> import pandas as pd
  >>> data = pd.DataFrame({"型号":[1,2,3,4],"录取":['清华','百大','蓝翔','瑞文'],'学历':['本科','专科','没有','启动']})
  >>> data
     型号  录取  学历
  0   1  清华  本科
  1   2  百大  专科
  2   3  蓝翔  没有
  3   4  瑞文  启动
  >>> pd.get_dummies(data)
     型号  录取_清华  录取_瑞文  录取_百大  录取_蓝翔  学历_专科  学历_启动  学历_本科  学历_没有
  0   1   True  False  False  False  False  False   True  False
  1   2  False  False   True  False   True  False  False  False
  2   3  False  False  False   True  False  False  False   True
  3   4  False   True  False  False  False   True  False  False
  ```

  

  ##### 2. 根据编码后的类别，反向推导出编码前对应的原始标签 

  ##### 方法：inverse_transform

```
#根据编码后的类别，反向推导出编码前对应的原始标签
print(enc.inverse_transform([0,1,2]))
 
 
输出：['兔子' '小狗' '小猫']
```

---

data_tr.columns.tolist()

将列名转化为列表

---

这个函数，是用来分割训练集和测试集的

model_selection.train_test_split

参考：[(12条消息) sklearn.model_selection中train_test_split()函数_丿一叶秋丶的博客-CSDN博客](https://blog.csdn.net/zhuqiang9607/article/details/83686308)

---

随机森林类

RandomForestClassifier

参考：[(12条消息) sklearn库学习----随机森林(RandomForestClassifier，RandomForestRegressor)_from sklearn.ensemble import_iostreamzl的博客-CSDN博客](https://blog.csdn.net/weixin_43776305/article/details/116895875)

---

GridSearchCV的名字其实可以拆分为两部分，GridSearch和CV，即网格搜索和交叉验证

best_score是最好的分数(交叉验证中设置的那个scoring = 'roc_auc'或者是'recall')



best_estimator_是网格搜索找到的最好的参数，并且把参数加载到了那个模型里了，这个best_estimator_就是最好的模型

model = grid_cv.best_estimator_
这个最好的模型可以用来预测，打分，就是和我们原来学的那些模型一样的用法，所以这个best_estimator_.score算出来的是最好模型下的准确率

*class* `sklearn.model_selection.``GridSearchCV`(*estimator*, *param_grid*, *scoring=None*, *fit_params=None*, *n_jobs=1*, *iid=True*, *refit=True*, *cv=None*, *verbose=0*, *pre_dispatch=‘2\*n_jobs’*, *error_score=’raise’*, *return_train_score=’warn’*)

1.  estimator：所使用的分类器，比如：estimator=RandomForestClassifier(min_samples_split=100, min_samples_leaf=20, max_depth=8, max_features='sqrt', random_state=10)，并且传入除需要确定最佳的参数之外的其他参数。每个分类器都需要一个scoring参数或者score方法。
2. param_grid：值为字典或列表，即需要最优化的参数的取值，param_grid =param_test1，param_test1 = {'n_estimators':range(10,71,10)}
3. scoring：准确评价标准，默认为None（使用estimator的误差估计函数），这时需要使用score函数；或者如scoring='roc_auc'，根据所选模型不同，评价准则不同。
4. cv：交叉验证参数，默认为None
5. refit：默认为True，程序将会以交叉验证训练集得到的最佳参数，重新对所有可用的训练集与测试集进行，作为最终用于性能评估的最佳模型参数。即在搜索参数结束后，用最佳参数结果再次fit一遍全部数据集。
6. iid:默认True,为True时，默认为各个样本fold概率分布一致，误差估计为所有样本之和，而非各个fold的平均。
7. verbose：日志冗长度，int：冗长度，0：不输出训练过程，1：偶尔输出，>1：对每个子模型都输出。
8. n_jobs: 并行数，int：个数,-1：跟CPU核数一致, 1:默认值。
9. pre_dispatch：指定总共分发的并行任务数。当n_jobs大于1时，数据将在每个运行点进行复制，这可能导致OOM，而设置pre_dispatch参数，则可以预先划分总共的job数量，使数据最多被复制pre_dispatch次，进行预测的常用方法和属性
10. grid.fit()：运行网格搜索
11. grid_scores_：给出不同参数情况下的评价结果
12. best_params_：描述了已取得最佳结果的参数的组合
13. best_score_：成员提供优化过程期间观察到的最好的评分

---

完善代码

````
#通过Pandas中的get_dummies() 进行编码
data_one_dummy = pd.get_dummies(data_tr[data_columns_one])
data_one_dummy_list = data_one_dummy.columns.tolist()
data_one_dummy_list

#开始预测
pred = rf1.predict(X_test_one)
pred_df = pd.DataFrame(pred, columns=['survived'])
pred_df.head(10)
````